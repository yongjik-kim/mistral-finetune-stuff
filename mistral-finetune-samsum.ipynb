{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:87: RequestsDependencyWarning: urllib3 (2.1.0) or chardet (4.0.0) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "\n",
    "fsdp_plugin = FullyShardedDataParallelPlugin(\n",
    "    state_dict_config=FullStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    "    optim_state_dict_config=FullOptimStateDictConfig(offload_to_cpu=True, rank0_only=False),\n",
    ")\n",
    "\n",
    "accelerator = Accelerator(fsdp_plugin=fsdp_plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'dialogue', 'summary'],\n",
      "    num_rows: 14732\n",
      "})\n",
      "Dataset({\n",
      "    features: ['id', 'dialogue', 'summary'],\n",
      "    num_rows: 818\n",
      "})\n",
      "Dataset({\n",
      "    features: ['id', 'dialogue', 'summary'],\n",
      "    num_rows: 819\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset('samsum', split='train')\n",
    "eval_dataset = load_dataset('samsum', split='validation')\n",
    "test_dataset = load_dataset('samsum', split='test')\n",
    "\n",
    "print(train_dataset)\n",
    "print(eval_dataset)\n",
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a14c56152c54f00a544ed05ed5519a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    cache_dir=\"models\"\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_id, quantization_config=bnb_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    model_max_length=512,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize(prompt):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "# I'm giving a very simple prompt to see fine-tuning effect more clearly.\n",
    "# With good enough prompt, Mistral-7B base model should be able to do summarization already pretty well.\n",
    "def generate_and_tokenize_prompt(data_point):\n",
    "    full_prompt =f\"\"\"\n",
    "### Dialogue:\n",
    "{data_point[\"dialogue\"]}\n",
    "\n",
    "### Summary:\n",
    "{data_point[\"summary\"]}\n",
    "\"\"\"\n",
    "    return tokenize(full_prompt)\n",
    "\n",
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dialogue: Eric: MACHINE!\n",
      "Rob: That's so gr8!\n",
      "Eric: I know! And shows how Americans see Russian ;)\n",
      "Rob: And it's really funny!\n",
      "Eric: I know! I especially like the train part!\n",
      "Rob: Hahaha! No one talks to the machine like that!\n",
      "Eric: Is this his only stand-up?\n",
      "Rob: Idk. I'll check.\n",
      "Eric: Sure.\n",
      "Rob: Turns out no! There are some of his stand-ups on youtube.\n",
      "Eric: Gr8! I'll watch them now!\n",
      "Rob: Me too!\n",
      "Eric: MACHINE!\n",
      "Rob: MACHINE!\n",
      "Eric: TTYL?\n",
      "Rob: Sure :)\n",
      "Summary: Eric and Rob are going to watch a stand-up on youtube.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Dialogue:\n",
      "Jennifer: The new Marvel movies haven't been doing well, and I am pretty sure the new one also sucks.\n",
      "Lee: But I really wanted to see how Miss Marvel would look in theater!\n",
      "Zain: Well, my friend have seen Miss Marvel movie already, and they said that it really sucked. That's even considering them having all the comic books too.\n",
      "Lee: Huh... Well, I guess I'll see it by myself when it hits the VOD. So, which one should we watch now?\n",
      "Jennifer: There's tickets to Avatar 2 in about an hour. We could get those and watch that instead.\n",
      "Lee: Sounds good to me. I really liked the first one anyways.\n",
      "Zain: I'm on board also. Let's see that one instead! I'll buy the popcorn.\n",
      "\n",
      "### Summary:\n",
      "The new Marvel movies haven't been doing well, and I am pretty sure the new one also sucks. But I really wanted to see how Miss Marvel would look in theater! But my friend have seen Miss Marvel movie already, and they said that it really sucked. That's even considering them having all the comic books too. Well, I guess I'll see it by myself when it hits the VOD. So, which one should we watch now? There's tickets to Avatar 2 in about an hour. We could get those and watch that instead. I really liked the first one anyways. I'm on board also. Let's see that one instead! I'll buy the popcorn.\n",
      "\n",
      "### Vocabulary:\n",
      "- **sucks**: (v) to be bad or not good\n",
      "- **sucked**: (v) to be bad or not good\n",
      "- **sucking**: (v) to be bad or not good\n",
      "- **sucked**: (v) to be bad or not good\n",
      "- **sucked**: (v) to be bad or not good\n",
      "- **sucked**: (v) to be bad or not good\n",
      "- **sucked**: (v\n"
     ]
    }
   ],
   "source": [
    "print(\"Dialogue: \" + test_dataset[1]['dialogue'])\n",
    "print(\"Summary: \" + test_dataset[1]['summary'] + \"\\n\")\n",
    "\n",
    "eval_prompt = \"\"\"\n",
    "### Dialogue:\n",
    "Jennifer: The new Marvel movies haven't been doing well, and I am pretty sure the new one also sucks.\n",
    "Lee: But I really wanted to see how Miss Marvel would look in theater!\n",
    "Zain: Well, my friend have seen Miss Marvel movie already, and they said that it really sucked. That's even considering them having all the comic books too.\n",
    "Lee: Huh... Well, I guess I'll see it by myself when it hits the VOD. So, which one should we watch now?\n",
    "Jennifer: There's tickets to Avatar 2 in about an hour. We could get those and watch that instead.\n",
    "Lee: Sounds good to me. I really liked the first one anyways.\n",
    "Zain: I'm on board also. Let's see that one instead! I'll buy the popcorn.\n",
    "\n",
    "### Summary:\n",
    "\"\"\"\n",
    "\n",
    "# Re-init the tokenizer so it doesn't add padding or eos token\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    add_bos_token=True,\n",
    ")\n",
    "\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(model.generate(**model_input, max_new_tokens=256)[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 21260288 || all params: 3773331456 || trainable%: 0.5634354746703705\n"
     ]
    }
   ],
   "source": [
    "from peft import prepare_model_for_kbit_training\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=8,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,  # Conventional\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "# Apply the accelerator. You can comment this out to remove the accelerator.\n",
    "model = accelerator.prepare_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "project = \"samsum-finetune\"\n",
    "base_model_name = \"mistral\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=5,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_accumulation_steps=4,\n",
    "        gradient_checkpointing_kwargs={'use_reentrant':False},\n",
    "        max_steps=2000,\n",
    "        learning_rate=2.5e-5, # Want about 10x smaller than the Mistral learning rate\n",
    "        logging_steps=100,\n",
    "        bf16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_dir=\"./logs\",        # Directory for storing logs\n",
    "        save_strategy=\"steps\",       # Save the model checkpoint every logging step\n",
    "        save_steps=100,                # Save checkpoints every 50 steps\n",
    "        evaluation_strategy=\"steps\", # Evaluate the model every logging step\n",
    "        eval_steps=100,               # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=True,                # Perform evaluation at the end of training\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yjkim/.local/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:472: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a870e6754d64a0c94a33849e978de05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    cache_dir=\"models\"\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_id,  # Mistral, same as before\n",
    "    quantization_config=bnb_config,  # Same quantization config as before\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    use_auth_token=True\n",
    ")\n",
    "\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_id,\n",
    "    add_bos_token=True,\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "from peft import PeftModel\n",
    "\n",
    "ft_model = PeftModel.from_pretrained(base_model, \"mistral-samsum-finetune/checkpoint-2000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Dialogue:\n",
      "Jennifer: The new Marvel movies haven't been doing well, and I am pretty sure the new one also sucks.\n",
      "Lee: But I really wanted to see how Miss Marvel would look in theater!\n",
      "Zain: Well, my friend have seen Miss Marvel movie already, and they said that it really sucked. That's even considering them having all the comic books too.\n",
      "Lee: Huh... Well, I guess I'll see it by myself when it hits the VOD. So, which one should we watch now?\n",
      "Jennifer: There's tickets to Avatar 2 in about an hour. We could get those and watch that instead.\n",
      "Lee: Sounds good to me. I really liked the first one anyways.\n",
      "Zain: I'm on board also. Let's see that one instead! I'll buy the popcorn.\n",
      "\n",
      "### Summary:\n",
      "  Lee, Jennifer and Zain will watch Avatar 2 instead of Miss Marvel. Zain will buy the popcorn.\n",
      "\n",
      "### Analysis:\n",
      "Lee, Jennifer and Zain will watch Avatar 2 instead of Miss Marvel. Zain will buy the popcorn.\n",
      "\n",
      "### Special notes:\n",
      "\n",
      "\n",
      "### Comparison:\n",
      "\n",
      "\n",
      "### Summary:\n",
      "Lee, Jennifer and Zain will watch Avatar 2 instead of Miss Marvel. Z\n"
     ]
    }
   ],
   "source": [
    "eval_prompt = \"\"\"\n",
    "### Dialogue:\n",
    "Jennifer: The new Marvel movies haven't been doing well, and I am pretty sure the new one also sucks.\n",
    "Lee: But I really wanted to see how Miss Marvel would look in theater!\n",
    "Zain: Well, my friend have seen Miss Marvel movie already, and they said that it really sucked. That's even considering them having all the comic books too.\n",
    "Lee: Huh... Well, I guess I'll see it by myself when it hits the VOD. So, which one should we watch now?\n",
    "Jennifer: There's tickets to Avatar 2 in about an hour. We could get those and watch that instead.\n",
    "Lee: Sounds good to me. I really liked the first one anyways.\n",
    "Zain: I'm on board also. Let's see that one instead! I'll buy the popcorn.\n",
    "\n",
    "### Summary:\n",
    "\"\"\"\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "ft_model.eval()\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(ft_model.generate(**model_input, max_new_tokens=100)[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
